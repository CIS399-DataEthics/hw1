{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 399 Homework 1: Machine Learning Basics and Overfitting\n",
    "\n",
    "## (replace this line with your name)\n",
    "\n",
    "In this homework, you will gain experience with basic tools for training and evaluating machine learning models. This assignment makes use of Python 3, as well as standard Python libraries for data science such as pandas, scikit-learn, and matplotlib. If this is your first time programming in Python, you may want to examine the language documentation (https://docs.python.org/3/), which includes some simple tutorials.\n",
    "\n",
    "All cells where code is required are marked with a \"YOUR CODE HERE\" comment. The point values for each code block are written in the header for the associated subsection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data\n",
    "\n",
    "Pandas DataFrames are useful structures for working with data in Python. We can load our data from a CSV file directly into a DataFrame and display a sample of rows as output.\n",
    "\n",
    "The data we are using for this homework is from the \"Communities and Crime\" dataset available from UC Irvine's Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/communities+and+crime). It includes data about the different types of crimes among various communities, socioeconomic and racial data about each community, and information about the police force in each community.\n",
    "\n",
    "The last column indicates whether or not there is a high rate of violent crime in the community (1 if yes, 0 if no). This is the target (Y) variable for the dataset. Running the two cells below will display a subset of the entries as well as a list of the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataframe = pd.read_csv(\"communities.csv\")\n",
    "\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the numerical values from a DataFrame into a Numpy array. Depending on the situation, these formats have various strengths and weaknesses. Numpy arrays are lightweight and behave much like a standard list, but do not support heterogenous data or many of the Pandas features for indexing and querying.\n",
    "\n",
    "Below, we extract the values from our dataframe, display the dimensions of the array, and display a subset of the rows and columns. You should find that the values displayed match with the above dataframe output.\n",
    "\n",
    "More information about array indexing in Numpy is available here: https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe.values[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Creating X and Y (5 Points)\n",
    "\n",
    "Create arrays titled \"X\" and \"y\", where X consists of all but the last column of data (for all rows) and y is exclusively the last column. Print the shape of each array as accessed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Creating Train and Test Sets (5 points)\n",
    "\n",
    "Create arrays titled: \n",
    "- X_train (first 1000 rows of X)\n",
    "- X_test (remaining rows of X)\n",
    "- y_train (first 1000 rows of y)\n",
    "- y_test (remaining rows of y)\n",
    "\n",
    "As the order of the records in the dataset are randomized, it is fine to simply use the beginning of the file as training and the rest as test.\n",
    "\n",
    "For the y arrays, you may want to consider using the .ravel() method for \"flattening\" the array from 2 dimensions down to 1. Print the shape of each array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training a Model\n",
    "\n",
    "Scikit-learn features modules for a wide variety of machine learning algorithms, such as the ones we've been discussing in class. Read the documentation to understand how to train these models and generate predictions.\n",
    "- Logistic Regression documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Decision Tree documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, tree\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(max_depth=5)\n",
    "logreg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Making Predictions with Decision Trees and Linear Regression (10 points)\n",
    "\n",
    "Using your X_train and y_train arrays, train:\n",
    "- a Decision Tree model\n",
    "- a linear model via Logistic Regression (this may throw a DataConversionWarning which you can ignore)\n",
    "\n",
    "Using x_test, generate \"y_hat\" predictions (one set of predictions with each model). Print the shape of each prediction array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluating a Model (5 points)\n",
    "\n",
    "Write a function which takes in 2 binary arrays as arguments (i.e. y and y_hat) and computes the prediction error as a decimal between 0 and 1. Use this function to compute the errors for your Decision Tree and Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def error(y, y_hat):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(error(y_test, y_hat_dt), error(y_test, y_hat_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing Tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using matplotlib\n",
    "\n",
    "Matplotlib is a powerful library for graphing and data visualization in Python. Below we demonstrate some of its features in a generic plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Sample 100 random values from [0,1]\n",
    "y1_example = np.array([random.random() for i in range(100)])\n",
    "y2_example = np.array([i *0.01 for i in range(100)])\n",
    "# Create an array with the indices\n",
    "x_example = np.array(range(len(y1_example)))\n",
    "\n",
    "# Create a plot with a caption, X and Y legends, etc\n",
    "x_label = 'X value'\n",
    "y_label = 'Y value'\n",
    "plt.title('Example Plot')\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "\n",
    "\n",
    "plt.scatter(x_example, y1_example, color='red', label='Points')\n",
    "plt.plot(x_example, y1_example, color='blue', label='Line 1')\n",
    "plt.plot(x_example, y2_example, color='green', label='Line 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Sample Size vs Generalization Error (10 points)\n",
    "\n",
    "Write code which creates training sets of size $n \\in \\{10,20,...,990,1000\\}$ by taking the first $n$ rows of X_train and y_train (give these different names from the original arrays). Train Decision Tree and Logistic Regression models with each of these training sets, generate out-of-sample predictions using X_test, and compute error using y_train as above.\n",
    "\n",
    "Generate a matplotlib plot with \"Sample Size\" as the X-axis and \"Test Error\" as the Y-axis. Plot lines for both the Decision Tree and Logistic Regression results. Plot your lines in different colors and include a legend to specify which line belongs to which model class. \n",
    "\n",
    "__Disclaimer:__ The results you will see show that test error decreases quickly the number of samples in the dataset. Often, in practice, it takes tens of thousands of training samples to see a meaningful decrease in test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Model Complexity vs Generalization Error (10 points)\n",
    "\n",
    "Vary the max depth of the decision tree from 1 to 15. Plot the resulting error when training a model with all 1000 rows of X_train and y_train. You can adjust the max depth by reinstantiating the DecisionTreeClassifier module with a max_depth parameter:\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(max_depth=i)\n",
    "\n",
    "Generate a plot with \"Max Depth\" as the X-axis and \"Test Error\" as the Y-axis. Plot the error when predicting labels for X_train as well as X_test for each value of the maximum tree depth. Plot your lines in different colors and include a legend to specify which line belongs to which model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Observing Error Disparities\n",
    "\n",
    "In this section, you will explore the disparities in error for different \"groups\" of the dataset. The error disparity between two test sets with errors $\\epsilon_1$ and $\\epsilon_2$ is $|\\epsilon_1 - \\epsilon_2|$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Splitting by Feature Values (10 points)\n",
    "\n",
    "Write a function which takes in X and y arrays, a column number, and a threshold. The function should return arrays X0 and y0 containing all rows where the value in the specified column falls strictly below the threshold, as well as arrays X1 and y1 containing all rows where the the value in the specified column is above or equal to the threshold. \n",
    "\n",
    "\n",
    "Numpy supports indexing via an array of values, which allows you to extract a non-contiguous subset of rows from an array. You might find this helpful. More information is available here: https://docs.scipy.org/doc/numpy-1.10.0/user/basics.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def split_on_feature(X_test, y_test, column, thresh):\n",
    "    \n",
    "    return (X0_test, X1_test, y0_test, y1_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Calculating All Discrepancies (10 points)\n",
    "\n",
    "Now, let's evaluate the error disparities for the model you previously trained in Section 2.1. If you used the same naming conventions for sections 3 and 4, the models may have been overwritten. If that's the case, make sure to rerun the code in Section 2.1. \n",
    "\n",
    "For each feature in the dataset, use the function from 5.1 to split on that column when the threshold is set to 0.5. Then compute the error disparity for the feature by calculating the error of predictions made on both X0 and X1. \n",
    "\n",
    "This cell should print out the columns _by name_ (using the list of names in the Pandas dataframe) along with their corresponding error discrepancies, and should print in descending order of error discrepancy. You should omit columns where either of the splits have fewer than 100 rows.\n",
    "\n",
    "__Before running any code__, look through the available features on the dataset, available at http://archive.ics.uci.edu/ml/datasets/communities+and+crime, and write down two attributes that you would expect to have __high__ error disparity, and two attributes you would expect to have __low__ error disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*click here to enter your answers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Other Types of Discrepancies (10 points)\n",
    "\n",
    "Instead of error disparities, let's compute two other types of errors that are of interest to us: False Negative Disparity and False Positive Disparity. \n",
    "\n",
    "For the feature racePctblack (percentage of population that is African-American), which is in column 2, compute the False Positive rate and False Negative rate using the provided functions. You should threshold the feature at 0.5 as earlier to create the two sets of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS:\n",
    "# y - true labels\n",
    "# y_hat - predicted labels\n",
    "def fp_error(y, y_hat):\n",
    "    fp_errors = [np.maximum(y_hat[i] - y[i], 0) for i in range(len(y))]\n",
    "    return np.mean(fp_errors)\n",
    "\n",
    "## INPUTS:\n",
    "# y - true labels\n",
    "# y_hat - predicted labels\n",
    "def fn_error(y, y_hat):\n",
    "    fn_errors = [np.maximum(y[i] - y_hat[i], 0) for i in range(len(y))]\n",
    "    return np.mean(fn_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "print('False Positive Error Rate of Communities with Above Median Black Population: ', y1_fperr)\n",
    "print('False Positive Error Rate of Communities with Below Median Black Population: ', y0_fperr)\n",
    "\n",
    "print('False Negative Error Rate of Communities with Above Median Black Population: ', y1_fnerr)\n",
    "print('False Negative Error Rate of Communities with Below Median Black Population: ', y0_fnerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Short Response Questions (25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: When training a machine learning model with some dataset, what are some assumptions we are making about the data? What are some things that it is important for us not to assume? Please give a few examples for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*click here to enter your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Why is it important to evaluate our model on data which was not used in training? What is the error rate on \"test\" or \"holdout\" data supposed to be a proxy for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*click here to enter your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: In your own words, explain the results of your plot from 4.1. Why does it make sense that these results occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*click here to enter your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: In your own words, explain the results of your plot from 4.2. Why does it make sense that these results occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*click here to enter your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: In your own words, explain the results of section 5.3. What are some possible implications of this model in terms of unfairness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*click here to enter your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Extra Credit (5-10 points)\n",
    "\n",
    "Play around with the data and generate some kind of plot (via matplotlib) that you find interesting. Write a few sentences about your process, what you found, and what you think it suggests about the data. This could be an evaluation of multiple model classes, a statistical analysis of different features, unsupervised analysis, extending the investigation into error discrepancies, or anything else you can think of. \n",
    "\n",
    "Any well-justified solution will earn up to 5 points of extra credit. The 3 submissions we deem most interesting will earn up to 10 points of extra credit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
